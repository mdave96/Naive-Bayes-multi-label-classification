{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Reuters_multi-label_classification.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbCPf64Lgw08",
        "colab_type": "text"
      },
      "source": [
        "## Multi-Label classification using Naive-Bayes\n",
        "Reuters dataset is used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-19Zbjfhgw1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Importing necessary packages\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from skmultilearn.problem_transform import LabelPowerset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKDIXj3xgw1p",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocesssing Data\n",
        "We make use of *Beautiful Soup* from *bs4* package to read the SGML files and extract necessary attributes from it's html tags.\n",
        "<br> Below function takes the output from *Reuters* tag for a single SGML file and returns a dataframe with all the necessary attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBmme5nUgw1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_to_df(reuters):\n",
        "    topics = []\n",
        "    titles = []\n",
        "    body = []\n",
        "\n",
        "    for i in range(len(reuters)):\n",
        "        tp = reuters[i]('topics')\n",
        "        tt = reuters[i]('title')\n",
        "        bd = reuters[i]('text')\n",
        "    \n",
        "        ## Titles\n",
        "        if tt == []:\n",
        "            titles.append('')\n",
        "        else:\n",
        "            titles.append(str(tt[0].contents[0].string))\n",
        "    \n",
        "        ## Topics\n",
        "        if len(tp[0]) == 0:\n",
        "            temp = ''\n",
        "        else:\n",
        "            temp = []\n",
        "            for j in range(len(tp[0])):\n",
        "                temp.append(str(tp[0].contents[j].string))\n",
        "        topics.append(temp)\n",
        "    \n",
        "        ## Body\n",
        "        if len(bd) == 0:\n",
        "            body.append('')\n",
        "        else:\n",
        "            body.append(str(bd[0].contents[-1]))\n",
        "        \n",
        "    df = pd.DataFrame({'topics' : topics, 'title' : titles, 'body' : body})\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnXXa-rCgw2L",
        "colab_type": "text"
      },
      "source": [
        "#### Loading Data and constructing a DataFrame\n",
        "Below code iterates over the 22 SGML files in the Dataset, and builds a dataframe for each file and appends it to the previously formed DataFrame, reulting into a single DataFrame of 21578 rows (indicating the no. of articles) and 3 columns namely *Topics, Title, Body*.\n",
        "<br>The .sgm file is passed in *BeautifulSoup()* as an argument and this is in turn is used to extract all tags named *reuters* associated with each article. This is then passed on to the function defined above to get DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngR6vVu0gw2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame()\n",
        "\n",
        "for i in range(0,22):\n",
        "    file_name = \"reut2-\" + '%03d' % i + '.sgm'\n",
        "    with open(file_name) as file:\n",
        "        soup = BeautifulSoup(file)\n",
        "        reuters = soup('reuters')\n",
        "        df = extract_to_df(reuters)\n",
        "        data = data.append(df, ignore_index = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaJQKwgbgw2o",
        "colab_type": "text"
      },
      "source": [
        "#### Cleaning Data\n",
        "We want to predict the *Topics* of an article and there are a lot of articles in the dataset that don't have any topic associated with them. <br> We choose to remove such observavtions because, for using Naive Bayes Classification method, the generative model is probabilistic. If there is no *Topic* associated with a row, it creates a problem in fitting the model.<br>\n",
        "<br> Also, we make another column that contains merged string of attributes *Title* and *Body* of the article, this is because we make use of *Tfidf vectorizer* ahead which requires the data be passed as an numpy array of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZu4QIAqgw2t",
        "colab_type": "code",
        "colab": {},
        "outputId": "2ef7ab51-f460-4a14-8e62-639a690ba52d"
      },
      "source": [
        "data = data[data['topics'] != '']\n",
        "data['text'] = data['title']\n",
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topics</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>11367</td>\n",
              "      <td>11367</td>\n",
              "      <td>11367</td>\n",
              "      <td>11367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>655</td>\n",
              "      <td>10875</td>\n",
              "      <td>10316</td>\n",
              "      <td>10875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>[earn]</td>\n",
              "      <td></td>\n",
              "      <td>Blah blah blah.\\n\\n\\n</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>3945</td>\n",
              "      <td>62</td>\n",
              "      <td>818</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        topics  title                   body   text\n",
              "count    11367  11367                  11367  11367\n",
              "unique     655  10875                  10316  10875\n",
              "top     [earn]         Blah blah blah.\\n\\n\\n       \n",
              "freq      3945     62                    818     62"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce4ybc2Vgw3K",
        "colab_type": "text"
      },
      "source": [
        "#### Attribute transformation\n",
        "The target attribte *Topics* is such that each article can have one or more *Topics*. This means it is a *Multi-Label* attribute, not just a Multi-class attribute. <br>\n",
        "We use a method called *MultiLabelBinarizer* which takes all the possible *Topics* there are in the attribute, and makes an array of 0's and 1's for each row, indicating whether *i'th* position *Topic* is in that row's *Topic* column or not. <br>\n",
        "There are 120 distinct *Topics* so the array size will be of 120 and very few of them will be 1's. <br>\n",
        "This method transforms the *Topics* attribute into a numpy array of arrays of 0's and 1's and stores it as a Sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPiCmXJQgw3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(data['topics'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLEIr38Cgw3q",
        "colab_type": "text"
      },
      "source": [
        "#### TFIDF Vectorizer\n",
        "The independent attribute of data is a string that contains the *Title* and *Body* of the article merged together in a string. <br><br>\n",
        "TFIDF - Term Frequency-Inverse Docment Frequency <br>\n",
        "Tfidf vectorization is the transformation of text into a meaningul format in terms of frequency (and then invere doc freq) of distinct words occuring in the text. It firsts counts the frequency of each word that occurs in the text. Now there are some words that are common to almost all articles and occur almost everwhere like *a, the, of, from, for, etc.* <br>\n",
        "That is, equal weights are given to common but unimportant words and those words that are quite important but not as common as above words. <br><br>\n",
        "Tfidf then calculates a term called the Inverse Document Frequency which is the log of ratio of #of docs to #of docs containing the particular word. And then the TFIDF is calculates as the multiplication of *Term Freq * Inverse Doc Freq*. <br>\n",
        "Thus, each row has a corpus of words and a TFIDF freq associated with it. i.e. Text transformed into a vector of words and frequencies associated with it, which are called the features of the Text. <br> <br>\n",
        "It might be possible that bigger the text, the number of features is very very large. This is not so desirable as with too many features, the fitted model might not generalise well. So we put a Max value to the #of features and it keeps that many best features and neglects the others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlQjnfGcgw3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vect = TfidfVectorizer(max_features = 5500)\n",
        "X = vect.fit_transform(data['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8ejcuzEgw4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Splitting the data X and y into training and testing sets with the testing size of our choice.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c-Crs32gw4R",
        "colab_type": "text"
      },
      "source": [
        "#### Fitting the model\n",
        "We fit the *ComplementNB* model from sklearn.naive_bayes package using the above Training Data. <br>\n",
        "But since an article can have one or more *Topics* assigned to it, we need a method that can do it. <br>The sklearn Naive_bayes predicts a label, but the *LabelPowerset* method from *sk-multilearn* library predicts multiple labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0WzzHkMgw4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = LabelPowerset(ComplementNB())\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad0PBKVfgw4v",
        "colab_type": "text"
      },
      "source": [
        "#### Cross-validation on Training Data\n",
        "We perform k-fold (here, 10 fold) cross-validation on the Training Data to check whether the model gives significantly distant apart accuracies which indicate the model is not a good fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdqKjOaggw41",
        "colab_type": "code",
        "colab": {},
        "outputId": "460307c9-c034-4368-f330-b66b5142c75b"
      },
      "source": [
        "cross_val_score(classifier, X=X_train, y=y_train, cv=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.77142857, 0.77802198, 0.77802198, 0.78767877, 0.779978  ,\n",
              "       0.78107811, 0.77777778, 0.76677668, 0.78437844, 0.78657866])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ycyky67gw5H",
        "colab_type": "text"
      },
      "source": [
        "Thus we can say that the Cross-validation results indicate a consistent behaviour of model in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e7rQ88Kgw5M",
        "colab_type": "text"
      },
      "source": [
        "#### Predicting topics and finding out accuracy\n",
        "We use the validated model to predict *Topics* on the Test dataset and check it's accuracy score. <br>\n",
        "The accuracy here is defined as the ratio of correctly classified topics to the total no. of observations.\n",
        "*Correctly classified topics* here means that if an article has 3 topics, then a correct classification includes predicting all the 3 topics correctly and hence matching the predicted topics to actual topics exactly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMpAbVyngw5R",
        "colab_type": "code",
        "colab": {},
        "outputId": "a081bb89-7f25-4f83-83d9-bb50475e9bea"
      },
      "source": [
        "predictions = classifier.predict(X_test)\n",
        "y_pred = mlb.inverse_transform(predictions)\n",
        "accuracy_score(y_test,predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7893579595426561"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqyTihJ3gw5g",
        "colab_type": "text"
      },
      "source": [
        "Thus, we get an accuracy of 78% on the Test Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhlmA120gw5k",
        "colab_type": "text"
      },
      "source": [
        "### Precision and Recall\n",
        "Precision and Recall are defined slight differently for Multi-Label Classification. We could not figure out the meaning of Confusion matrix for Multi-Label Classification as there are multiple classes involved and also multiple labels assigned to each instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVqUNH6Ogw5p",
        "colab_type": "text"
      },
      "source": [
        "#### Precision Score\n",
        "Definition on Sklearn documentation : *Calculate metrics for each instance, and find their average.* <br><br>\n",
        "Here, in Multi-Label classification, a score is calculated for each observation, which is the ratio - # of topics common in predicted and true sets to the # of predicted topics.<br>For n observations, n such scores are calculated and the average score is the Precision Score of it. <br>\n",
        "i.e. For an article, model predicted \"egg\", \"milk\" and \"yogurt\". And true value was \"milk\" and \"yogurt\". Then score for this observation will be 2/3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmHZ73YIgw5y",
        "colab_type": "code",
        "colab": {},
        "outputId": "d613e142-8bed-4f92-d23e-95956a7c2ef1"
      },
      "source": [
        "precision_score(y_test,predictions, average = 'samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8626397788666917"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paNwXad8gw6J",
        "colab_type": "text"
      },
      "source": [
        "#### Recall Score\n",
        "Similar setting to that of Precision for Multi-label classification.<br><br>\n",
        "A score is calculated for each observation, which is the ratio - # of topics common in predicted and true sets to the # of true topics. And then average of n scores is taken to be the Recall Score. (n is the # of observations present) <br>\n",
        "i.e. For an article, model predicted \"egg\", \"milk\" and \"yogurt\". And true value was \"milk\" and \"yogurt\". Then score for this observation will be 2/2 = 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfgNImzqgw6P",
        "colab_type": "code",
        "colab": {},
        "outputId": "146b7c28-6d35-435a-8288-e29620c818f8"
      },
      "source": [
        "recall_score(y_test, predictions, average = \"samples\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8372726352607619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMBwEHZXgw6f",
        "colab_type": "code",
        "colab": {},
        "outputId": "9537ae56-b5f9-4ee9-9b7d-ff9cf3dd0873"
      },
      "source": [
        "y_pred[:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('acq',),\n",
              " ('earn',),\n",
              " ('crude',),\n",
              " ('crude',),\n",
              " ('trade',),\n",
              " ('earn',),\n",
              " ('earn',),\n",
              " ('acq',),\n",
              " ('earn',),\n",
              " ('acq',),\n",
              " ('grain', 'wheat'),\n",
              " ('acq',),\n",
              " ('earn',),\n",
              " ('acq',),\n",
              " ('earn',),\n",
              " ('trade',),\n",
              " ('carcass', 'livestock'),\n",
              " ('acq',),\n",
              " ('interest', 'money-fx'),\n",
              " ('earn',)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}